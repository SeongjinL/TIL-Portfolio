# 멀티 클러스터 세팅

serverbase을 복제해서 server01(namenode,resourcemanager),02,03(datanode,nodemanager)생성

ip(192.168.10.101,192.168.10.102,103) 변경

```python
#ip 확인
$ cat /etc/netplan/00-installer-config.yaml
#ip 수정
$ sudo nano /etc/netplan/00-installer-config.yaml
```



hostname(server02,03) 변경

```python
#hostname 확인
$cat /etc/hostname
#hostname 수정
$sudo nano /etc/hostname
```



설정 후 리부팅

```python
$reboot
```

hadoop 설정(server01에만 우선 설정 후 복사)

```python
$nano hadoop/etc/hadoop/hadoop-env.sh 
export JAVA_HOME=/usr/local/java 
export HADOOP_PID_DIR=$HADOOP_HOME/pids

#namanode->server01, datanode->server02
$nano hadoop/etc/hadoop/core-site.xml
<configuration>      
	<property>              
		<name>fs.defaultFS</name>
		<value>hdfs://server01:9000</value>
	</property>
</configuration>

#hdfs 세팅(nameNode data경로, dataNode data경로 설정)
$nano hadoop/etc/hadoop/hdfs-site.xml
<configuration>  
	<property>          
		<name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/data/nameNode</value> 
	</property>  
	<property>          
		<name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/data/dataNode</value>  
		</property>  
		
		<property>          
			<name>dfs.replication</name>
            <value>2</value>
        </property>
</configuration>

#맵리듀스 세팅
$nano hadoop/etc/hadoop/mapred-site.xml
<configuration>  
	<property>          
		<name>mapreduce.framework.name</name>
    	<value>yarn</value>
        </property>
        <property>
        	<name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
</configuration>

#yarn 세팅(resourcemanager-> server01, nodemanager->server02,03)
$nano hadoop/etc/hadoop/yarn-site.xml
<configuration>
	<property>
    	<name>yarn.resourcemanager.hostname</name>
        <!-- resourcemanager server name -->
        <value>server01</value>
    </property>
    <property>
    	<name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
     </property>  
	 <property>        				
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>         <value>org.apache.hadoop.mapred.ShuffleHandler</value>  
	</property>
</configuration>

$nano hadoop/etc/hadoop/workers(datanode에 해당하는 것만 선택, namenode는 제외)
server02
server03

#02,03서버로 설정파일 복붙scp * 
server02:/home/hadoop/hadoop/etc/hadoopscp * 
server03:/home/hadoop/hadoop/etc/hadoop

#포맷
hdfs namenode -format

#hdfs, yarn 서비스 실행
$start-dfs.sh
$start-yarn.sh 
$start-all.sh #한번에 서비스 실행

#확인
$ssh server02 /usr/local/java/bin/jps (DataNode,NodeManager 실행중이면 성공)
$ssh server03 /usr/local/java/bin/jps (DataNode,NodeManager 실행중이면 성공)
#종료
$stop-all.sh
```



localhost:8970 -> pdfs web ui

Localhost:8088 -> resource manager web ui

```python
#소설 예제 다운
$wget https://www.gutenberg.org/cache/epub/2264/pg2264.txt
#파일을 hdfs로 올리기 $hdfs dfs -mkdir /user$hdfs dfs -mkdir/user/hadoop$hdfs dfs -mkdir input$hdfs dfs -put pg2264.txt input
#잘 올라갔나 확인
$hdfs dfs -cat input/pg2264.txtlocalhost:8970 웹 상에서도 확인 가능
#맵리듀스 처리(gi에 대한 빈도수 확인)
$cd hadoop$hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.2.jar grep input output 'gi[a-z.]+'
#output에서 결과 확인
```

```python
#추가 명령어 참고
#폴더 지우기
$hdfs dfs -rmr output 
```





